{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1974899852.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[54], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    import nltkfiltered_p1 = [i for i in p1_tokenized if not i.lower() in stop_words]\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import nltkfiltered_p1 = [i for i in p1_tokenized if not i.lower() in stop_words]\n",
    "print(filtered_p1)\n",
    "filtered_p2 = [i for i in p2_tokenized if not i.lower() in stop_words]\n",
    "print(filtered_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = '''Physical AI: The Fusion of Artificial Intelligence and Robotics\n",
    "Artificial Intelligence (AI) has revolutionized numerous fields, from data analysis to natural language processing. However, a new frontier in AI is emerging—**Physical AI**, which combines AI capabilities with robotics and material sciences to create intelligent systems that interact with the physical world. Unlike traditional AI, which primarily exists in software and digital environments, Physical AI integrates cognitive abilities with tangible, adaptable, and autonomous machines.'''\n",
    "\n",
    "\n",
    "p2 = '''Physical AI is the result of advancements in multiple disciplines, including machine learning, robotics, bioengineering, and nanotechnology. Early robots were programmed to perform specific tasks but lacked adaptability. With the rise of machine learning, these machines have become more autonomous and capable of learning from their environments. Additionally, innovations in soft robotics and smart materials have enabled the development of robots that can move and function more like living organisms.'''\n",
    "\n",
    "\n",
    "p3 = '''Several critical components define Physical AI:\n",
    "1. **Machine Learning and AI Algorithms** – These enable robots to perceive, analyze, and make decisions in real time.\n",
    "2. **Robotics and Mechatronics** – Combining mechanical design with AI allows for enhanced movement and interaction.\n",
    "3. **Soft Robotics** – The development of flexible and adaptive materials allows robots to move and respond more naturally.\n",
    "4. **Sensory Systems** – AI-driven robots use cameras, LiDAR, and haptic sensors to perceive and navigate their surroundings.\n",
    "5. **Autonomous Systems** – Machines equipped with self-learning capabilities can operate with minimal human intervention.'''\n",
    "\n",
    "p4 = '''Physical AI has applications across diverse industries:\n",
    "- **Healthcare** – AI-powered prosthetics, robotic-assisted surgeries, and eldercare robots provide medical support.\n",
    "- **Manufacturing** – Smart factories utilize autonomous robots to streamline production.\n",
    "- **Space Explor### The Evolution of Physical AIation** – AI-driven robots assist in planetary exploration and satellite maintenance.\n",
    "- **Disaster Response** – Robots equipped with AI can navigate hazardous environments for search and rescue missions.\n",
    "- **Everyday Assistance** – AI-powered robotic assistants help with household chores and personal care.'''\n",
    "\n",
    "\n",
    "p5 = '''Despite its promise, Physical AI presents challenges:\n",
    "- **Technical Limitations** – AI models require vast amounts of data and energy, and robots face difficulties in unpredictable environments.\n",
    "- **Ethical Concerns** – Questions arise about job displacement, privacy, and the moral implications of AI autonomy.\n",
    "- **Safety Risks** – AI-powered robots must be designed with fail-safes to prevent accidents.'''\n",
    "\n",
    "p6 = '''The future of Physical AI lies in enhancing adaptability, intelligence, and human-machine collaboration. Advances in **neuromorphic computing**, **biohybrid robotics**, and **AI-driven material innovation** will pave the way for machines that think, feel, and move like living beings. As these systems evolve, they will reshape industries, improve human lives, and redefine the boundaries between the digital and physical worlds.'''\n",
    "\n",
    "\n",
    "p7 = '''Physical AI represents the next step in artificial intelligence, merging computational power with real-world interaction. By bridging the gap between AI and robotics, this field holds the potential to revolutionize medicine, industry, space exploration, and daily life. However, with innovation comes responsibility—ensuring that Physical AI is developed ethically and safely will be key to its success.'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Physical', 'AI', ':', 'The', 'Fusion', 'of', 'Artificial', 'Intelligence', 'and', 'Robotics', 'Artificial', 'Intelligence', '(', 'AI', ')', 'has', 'revolutionized', 'numerous', 'fields', ',', 'from', 'data', 'analysis', 'to', 'natural', 'language', 'processing', '.', 'However', ',', 'a', 'new', 'frontier', 'in', 'AI', 'is', 'emerging—', '*', '*', 'Physical', 'AI', '*', '*', ',', 'which', 'combines', 'AI', 'capabilities', 'with', 'robotics', 'and', 'material', 'sciences', 'to', 'create', 'intelligent', 'systems', 'that', 'interact', 'with', 'the', 'physical', 'world', '.', 'Unlike', 'traditional', 'AI', ',', 'which', 'primarily', 'exists', 'in', 'software', 'and', 'digital', 'environments', ',', 'Physical', 'AI', 'integrates', 'cognitive', 'abilities', 'with', 'tangible', ',', 'adaptable', ',', 'and', 'autonomous', 'machines', '.']\n",
      "['Physical', 'AI', 'is', 'the', 'result', 'of', 'advancements', 'in', 'multiple', 'disciplines', ',', 'including', 'machine', 'learning', ',', 'robotics', ',', 'bioengineering', ',', 'and', 'nanotechnology', '.', 'Early', 'robots', 'were', 'programmed', 'to', 'perform', 'specific', 'tasks', 'but', 'lacked', 'adaptability', '.', 'With', 'the', 'rise', 'of', 'machine', 'learning', ',', 'these', 'machines', 'have', 'become', 'more', 'autonomous', 'and', 'capable', 'of', 'learning', 'from', 'their', 'environments', '.', 'Additionally', ',', 'innovations', 'in', 'soft', 'robotics', 'and', 'smart', 'materials', 'have', 'enabled', 'the', 'development', 'of', 'robots', 'that', 'can', 'move', 'and', 'function', 'more', 'like', 'living', 'organisms', '.']\n",
      "['Several', 'critical', 'components', 'define', 'Physical', 'AI', ':', '1', '.', '*', '*', 'Machine', 'Learning', 'and', 'AI', 'Algorithms', '*', '*', '–', 'These', 'enable', 'robots', 'to', 'perceive', ',', 'analyze', ',', 'and', 'make', 'decisions', 'in', 'real', 'time', '.', '2', '.', '*', '*', 'Robotics', 'and', 'Mechatronics', '*', '*', '–', 'Combining', 'mechanical', 'design', 'with', 'AI', 'allows', 'for', 'enhanced', 'movement', 'and', 'interaction', '.', '3', '.', '*', '*', 'Soft', 'Robotics', '*', '*', '–', 'The', 'development', 'of', 'flexible', 'and', 'adaptive', 'materials', 'allows', 'robots', 'to', 'move', 'and', 'respond', 'more', 'naturally', '.', '4', '.', '*', '*', 'Sensory', 'Systems', '*', '*', '–', 'AI-driven', 'robots', 'use', 'cameras', ',', 'LiDAR', ',', 'and', 'haptic', 'sensors', 'to', 'perceive', 'and', 'navigate', 'their', 'surroundings', '.', '5', '.', '*', '*', 'Autonomous', 'Systems', '*', '*', '–', 'Machines', 'equipped', 'with', 'self-learning', 'capabilities', 'can', 'operate', 'with', 'minimal', 'human', 'intervention', '.']\n",
      "['Physical', 'AI', 'has', 'applications', 'across', 'diverse', 'industries', ':', '-', '*', '*', 'Healthcare', '*', '*', '–', 'AI-powered', 'prosthetics', ',', 'robotic-assisted', 'surgeries', ',', 'and', 'eldercare', 'robots', 'provide', 'medical', 'support', '.', '-', '*', '*', 'Manufacturing', '*', '*', '–', 'Smart', 'factories', 'utilize', 'autonomous', 'robots', 'to', 'streamline', 'production', '.', '-', '*', '*', 'Space', 'Explor', '#', '#', '#', 'The', 'Evolution', 'of', 'Physical', 'AIation', '*', '*', '–', 'AI-driven', 'robots', 'assist', 'in', 'planetary', 'exploration', 'and', 'satellite', 'maintenance', '.', '-', '*', '*', 'Disaster', 'Response', '*', '*', '–', 'Robots', 'equipped', 'with', 'AI', 'can', 'navigate', 'hazardous', 'environments', 'for', 'search', 'and', 'rescue', 'missions', '.', '-', '*', '*', 'Everyday', 'Assistance', '*', '*', '–', 'AI-powered', 'robotic', 'assistants', 'help', 'with', 'household', 'chores', 'and', 'personal', 'care', '.']\n",
      "['Despite', 'its', 'promise', ',', 'Physical', 'AI', 'presents', 'challenges', ':', '-', '*', '*', 'Technical', 'Limitations', '*', '*', '–', 'AI', 'models', 'require', 'vast', 'amounts', 'of', 'data', 'and', 'energy', ',', 'and', 'robots', 'face', 'difficulties', 'in', 'unpredictable', 'environments', '.', '-', '*', '*', 'Ethical', 'Concerns', '*', '*', '–', 'Questions', 'arise', 'about', 'job', 'displacement', ',', 'privacy', ',', 'and', 'the', 'moral', 'implications', 'of', 'AI', 'autonomy', '.', '-', '*', '*', 'Safety', 'Risks', '*', '*', '–', 'AI-powered', 'robots', 'must', 'be', 'designed', 'with', 'fail-safes', 'to', 'prevent', 'accidents', '.']\n",
      "['The', 'future', 'of', 'Physical', 'AI', 'lies', 'in', 'enhancing', 'adaptability', ',', 'intelligence', ',', 'and', 'human-machine', 'collaboration', '.', 'Advances', 'in', '*', '*', 'neuromorphic', 'computing', '*', '*', ',', '*', '*', 'biohybrid', 'robotics', '*', '*', ',', 'and', '*', '*', 'AI-driven', 'material', 'innovation', '*', '*', 'will', 'pave', 'the', 'way', 'for', 'machines', 'that', 'think', ',', 'feel', ',', 'and', 'move', 'like', 'living', 'beings', '.', 'As', 'these', 'systems', 'evolve', ',', 'they', 'will', 'reshape', 'industries', ',', 'improve', 'human', 'lives', ',', 'and', 'redefine', 'the', 'boundaries', 'between', 'the', 'digital', 'and', 'physical', 'worlds', '.']\n",
      "['Physical', 'AI', 'represents', 'the', 'next', 'step', 'in', 'artificial', 'intelligence', ',', 'merging', 'computational', 'power', 'with', 'real-world', 'interaction', '.', 'By', 'bridging', 'the', 'gap', 'between', 'AI', 'and', 'robotics', ',', 'this', 'field', 'holds', 'the', 'potential', 'to', 'revolutionize', 'medicine', ',', 'industry', ',', 'space', 'exploration', ',', 'and', 'daily', 'life', '.', 'However', ',', 'with', 'innovation', 'comes', 'responsibility—ensuring', 'that', 'Physical', 'AI', 'is', 'developed', 'ethically', 'and', 'safely', 'will', 'be', 'key', 'to', 'its', 'success', '.']\n"
     ]
    }
   ],
   "source": [
    "p1_tokenized = word_tokenize(p1)\n",
    "print(p1_tokenized)\n",
    "p2_tokenized = word_tokenize(p2)\n",
    "print(p2_tokenized)\n",
    "p3_tokenized = word_tokenize(p3)\n",
    "print(p3_tokenized)\n",
    "p4_tokenized = word_tokenize(p4)\n",
    "print(p4_tokenized)\n",
    "p5_tokenized = word_tokenize(p5)\n",
    "print(p5_tokenized)\n",
    "p6_tokenized = word_tokenize(p6)\n",
    "print(p6_tokenized)\n",
    "p7_tokenized = word_tokenize(p7)\n",
    "print(p7_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Physical', 'AI', ':', 'Fusion', 'Artificial', 'Intelligence', 'Robotics', 'Artificial', 'Intelligence', '(', 'AI', ')', 'revolutionized', 'numerous', 'fields', ',', 'data', 'analysis', 'natural', 'language', 'processing', '.', 'However', ',', 'new', 'frontier', 'AI', 'emerging—', '*', '*', 'Physical', 'AI', '*', '*', ',', 'combines', 'AI', 'capabilities', 'robotics', 'material', 'sciences', 'create', 'intelligent', 'systems', 'interact', 'physical', 'world', '.', 'Unlike', 'traditional', 'AI', ',', 'primarily', 'exists', 'software', 'digital', 'environments', ',', 'Physical', 'AI', 'integrates', 'cognitive', 'abilities', 'tangible', ',', 'adaptable', ',', 'autonomous', 'machines', '.']\n",
      "['Physical', 'AI', 'result', 'advancements', 'multiple', 'disciplines', ',', 'including', 'machine', 'learning', ',', 'robotics', ',', 'bioengineering', ',', 'nanotechnology', '.', 'Early', 'robots', 'programmed', 'perform', 'specific', 'tasks', 'lacked', 'adaptability', '.', 'rise', 'machine', 'learning', ',', 'machines', 'become', 'autonomous', 'capable', 'learning', 'environments', '.', 'Additionally', ',', 'innovations', 'soft', 'robotics', 'smart', 'materials', 'enabled', 'development', 'robots', 'move', 'function', 'like', 'living', 'organisms', '.']\n",
      "['Several', 'critical', 'components', 'define', 'Physical', 'AI', ':', '1', '.', '*', '*', 'Machine', 'Learning', 'AI', 'Algorithms', '*', '*', '–', 'enable', 'robots', 'perceive', ',', 'analyze', ',', 'make', 'decisions', 'real', 'time', '.', '2', '.', '*', '*', 'Robotics', 'Mechatronics', '*', '*', '–', 'Combining', 'mechanical', 'design', 'AI', 'allows', 'enhanced', 'movement', 'interaction', '.', '3', '.', '*', '*', 'Soft', 'Robotics', '*', '*', '–', 'development', 'flexible', 'adaptive', 'materials', 'allows', 'robots', 'move', 'respond', 'naturally', '.', '4', '.', '*', '*', 'Sensory', 'Systems', '*', '*', '–', 'AI-driven', 'robots', 'use', 'cameras', ',', 'LiDAR', ',', 'haptic', 'sensors', 'perceive', 'navigate', 'surroundings', '.', '5', '.', '*', '*', 'Autonomous', 'Systems', '*', '*', '–', 'Machines', 'equipped', 'self-learning', 'capabilities', 'operate', 'minimal', 'human', 'intervention', '.']\n",
      "['Physical', 'AI', 'applications', 'across', 'diverse', 'industries', ':', '-', '*', '*', 'Healthcare', '*', '*', '–', 'AI-powered', 'prosthetics', ',', 'robotic-assisted', 'surgeries', ',', 'eldercare', 'robots', 'provide', 'medical', 'support', '.', '-', '*', '*', 'Manufacturing', '*', '*', '–', 'Smart', 'factories', 'utilize', 'autonomous', 'robots', 'streamline', 'production', '.', '-', '*', '*', 'Space', 'Explor', '#', '#', '#', 'Evolution', 'Physical', 'AIation', '*', '*', '–', 'AI-driven', 'robots', 'assist', 'planetary', 'exploration', 'satellite', 'maintenance', '.', '-', '*', '*', 'Disaster', 'Response', '*', '*', '–', 'Robots', 'equipped', 'AI', 'navigate', 'hazardous', 'environments', 'search', 'rescue', 'missions', '.', '-', '*', '*', 'Everyday', 'Assistance', '*', '*', '–', 'AI-powered', 'robotic', 'assistants', 'help', 'household', 'chores', 'personal', 'care', '.']\n",
      "['Despite', 'promise', ',', 'Physical', 'AI', 'presents', 'challenges', ':', '-', '*', '*', 'Technical', 'Limitations', '*', '*', '–', 'AI', 'models', 'require', 'vast', 'amounts', 'data', 'energy', ',', 'robots', 'face', 'difficulties', 'unpredictable', 'environments', '.', '-', '*', '*', 'Ethical', 'Concerns', '*', '*', '–', 'Questions', 'arise', 'job', 'displacement', ',', 'privacy', ',', 'moral', 'implications', 'AI', 'autonomy', '.', '-', '*', '*', 'Safety', 'Risks', '*', '*', '–', 'AI-powered', 'robots', 'must', 'designed', 'fail-safes', 'prevent', 'accidents', '.']\n",
      "['future', 'Physical', 'AI', 'lies', 'enhancing', 'adaptability', ',', 'intelligence', ',', 'human-machine', 'collaboration', '.', 'Advances', '*', '*', 'neuromorphic', 'computing', '*', '*', ',', '*', '*', 'biohybrid', 'robotics', '*', '*', ',', '*', '*', 'AI-driven', 'material', 'innovation', '*', '*', 'pave', 'way', 'machines', 'think', ',', 'feel', ',', 'move', 'like', 'living', 'beings', '.', 'systems', 'evolve', ',', 'reshape', 'industries', ',', 'improve', 'human', 'lives', ',', 'redefine', 'boundaries', 'digital', 'physical', 'worlds', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_p1 = [i for i in p1_tokenized if not i.lower() in stop_words]\n",
    "print(filtered_p1)\n",
    "filtered_p2 = [i for i in p2_tokenized if not i.lower() in stop_words]\n",
    "print(filtered_p2)\n",
    "filtered_p3 = [i for i in p3_tokenized if not i.lower() in stop_words]\n",
    "print(filtered_p3)\n",
    "filtered_p4 = [i for i in p4_tokenized if not i.lower() in stop_words]\n",
    "print(filtered_p4)\n",
    "filtered_p5 = [i for i in p5_tokenized if not i.lower() in stop_words]\n",
    "print(filtered_p5)\n",
    "filtered_p6 = [i for i in p6_tokenized if not i.lower() in stop_words]\n",
    "print(filtered_p6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/home/codespace/nltk_data'\n    - '/usr/local/python/3.12.1/nltk_data'\n    - '/usr/local/python/3.12.1/share/nltk_data'\n    - '/usr/local/python/3.12.1/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pos_tag\n\u001b[0;32m----> 3\u001b[0m tagged \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_p1\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/tag/__init__.py:168\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/tag/__init__.py:110\u001b[0m, in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    108\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger(lang\u001b[38;5;241m=\u001b[39mlang)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/tag/perceptron.py:183\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[0;34m(self, load, lang)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/tag/perceptron.py:273\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc \u001b[38;5;241m+\u001b[39m TAGGER_JSONS[lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/home/codespace/nltk_data'\n    - '/usr/local/python/3.12.1/nltk_data'\n    - '/usr/local/python/3.12.1/share/nltk_data'\n    - '/usr/local/python/3.12.1/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "tagged = nltk.pos_tag(filtered_p1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
